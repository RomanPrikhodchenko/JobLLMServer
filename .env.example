# =============================================================================
# LiteLLM Configuration
# =============================================================================

# Master API Key - MUST start with 'sk-' and be at least 32 characters
# Generate using: .\scripts\generate-keys.ps1
LITELLM_MASTER_KEY=sk-1234567890abcdefghijklmnopqrstuvwxyz

# Salt Key - CRITICAL: NEVER change after creating API keys!
# This is used to encrypt/decrypt virtual keys in the database
# If lost or changed, all existing API keys will become invalid
# Generate using: .\scripts\generate-keys.ps1
LITELLM_SALT_KEY=your-random-salt-key-min-43-characters-base64

# =============================================================================
# PostgreSQL Database Configuration
# =============================================================================

POSTGRES_DB=litellm
POSTGRES_USER=litellm
POSTGRES_PASSWORD=your-strong-postgres-password-min-32-chars

# =============================================================================
# LiteLLM Web UI Configuration
# =============================================================================

# Admin interface credentials for http://localhost:4000/ui
UI_USERNAME=admin
UI_PASSWORD=your-strong-ui-password-min-16-chars

# =============================================================================
# vLLM Model Configuration
# =============================================================================

# HuggingFace token for downloading models (required for gated models)
# Get it from: https://huggingface.co/settings/tokens
HF_TOKEN=hf_your_token_here

# Model to load (examples):
# - meta-llama/Llama-3.2-1B
# - meta-llama/Llama-3.2-3B
# - meta-llama/Meta-Llama-3-8B
# - mistralai/Mistral-7B-v0.1
VLLM_MODEL_NAME=meta-llama/Llama-3.2-1B

# Model name exposed in API (should match config.yaml)
SERVED_MODEL_NAME=llama-3-2-1b

# =============================================================================
# vLLM GPU Configuration
# =============================================================================

# Number of GPUs to use for tensor parallelism
# Set to 1 for single GPU, 2 for dual GPU, etc.
TENSOR_PARALLEL_SIZE=1

# GPU memory utilization (0.0 to 1.0)
# Recommended: 0.9 (90% of available GPU memory)
# Lower if you experience OOM errors
GPU_MEMORY_UTILIZATION=0.9

# Maximum sequence length (context window)
# Depends on model and available GPU memory
# Examples:
# - 4096 for most small models (1B-3B)
# - 8192 for medium models with sufficient VRAM
# - 32768 for models that support extended context
MAX_MODEL_LEN=4096

# =============================================================================
# Optional: Advanced vLLM Settings
# =============================================================================

# Uncomment and adjust as needed:
# MAX_NUM_BATCHED_TOKENS=8192
# MAX_NUM_SEQS=256
# ENABLE_CHUNKED_PREFILL=true
